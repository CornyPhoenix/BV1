% !TeX spellcheck = en_US
\documentclass[a4paper,twocolumn]{article}

\usepackage{fullpage}
\usepackage{fourier}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{tabularx}
\usepackage{todonotes}
\usepackage{verbatim}

%\titleformat{\subsection}[hang]{\large\bfseries}{\alph{subsection})\quad}{0pt}{}

\newcommand{\twodo}{\vspace{11pt}\textcolor{red}{\textbf{todo}}}
\newcommand{\subtask}[2]{\paragraph{#1)} \textit{#2} \newline}

\title{\textbf{Exercises for Image Processing 1}\\Problem Sheet 5}
\author{Axel Brand\\6145101 \and Nourhan Elfaramawy\\6517858 \and Sibel Toprak\\6712316}

\begin{document}
	\maketitle
	
	\section{Theoretical Problems}
	
	\subsection{(Lossless) Image Compression}
	
	\paragraph{a)} % What is the entropy of such documents?
	The so-called \textit{entropy} of such documents can be computed using following formula:
	\begin{align*}
	H = \sum_{g=0}^{G-1} P(g) \cdot \log_{2}\left(\frac{1}{P(g)}\right)
	\end{align*}
	Here, $G$ is the number of colors and $P(g)$ is the probability with which a particular color $g$ occurs in such a document. The following are the colors and their respective probabilities:
	\begin{center}
		\begin{tabular}{l r}
			$\mathbf{g}$ & $\mathbf{P(g)}$ \\
			\hline
			White  & 0.9   \\
			Black  & 0.08  \\
			Blue   & 0.012 \\
			Red    & 0.005 \\
			Green  & 0.002 \\
			Yellow & 0.001 \\
		\end{tabular}
	\end{center}
	Given these, we obtain the following intermediate results as well as the overall result for the entropy:
	\begin{align*}
	H
	&= 0.9 \cdot \log_{2}\left(\frac{1}{0.9}\right)
	&(\approx 0.13680) \\
	&+ 0.08 \cdot \log_{2}\left(\frac{1}{0.08}\right)
	&(\approx 0.29151) \\
	&+ 0.012 \cdot \log_{2}\left(\frac{1}{0.012}\right)
	&(\approx 0.07657) \\
	&+ 0.005 \cdot \log_{2}\left(\frac{1}{0.005}\right)
	&(\approx 0.03822) \\
	&+ 0.002 \cdot \log_{2}\left(\frac{1}{0.002}\right)
	&(\approx 0.01793) \\
	&+ 0.001 \cdot \log_{2}\left(\frac{1}{0.001}\right)
	&(\approx 0.00997) \\
	&\approx 0.570998
	\end{align*}
	
	Basically, this value tells how much information is contained in these documents. In general, the more uncertain or random the event is, the more information it will contain. Because, in 90\% of the cases the color will be white, there is no uncertainty, leading to this rather low entropy value. On the otherhand, the entropy with equally probable colors is equal to the number of bits required for coding. \footnote{\url{https://simple.wikipedia.org/wiki/Information_entropy}}
	
	\vspace{12pt}
	\todo[inline]{Sibel: Will rephrase this paragraph!}
	
	\begin{comment}
		Very intuitive example from "Simple Wikipedia":
		
		Let's look at an example. If someone is told something they already know, the information they get is very small. It will be pointless for them to be told something they already know. This information would have very low entropy.
		
		If they were told about something they knew little about, they would get much new information. This information would be very valuable to them. They would learn something. This information would have high entropy.
	\end{comment}
	
	\paragraph{b)} % Design a Huffman code for the pixels.
	See \Cref{fig:huffman_code}.
	\begin{figure*}[t]
		\begin{minipage}{0.45\textwidth}
			%\includegraphics[width=\linewidth]{imagefile}
			\todo[inline]{Sibel: Image will be added soon!}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\begin{tabular}{l r r c}
				\textbf{Color} & \textbf{Probability} & \textbf{Code} & \textbf{Length}\\
				\hline
				White & 0.9 & 0 & 1 \\
				Black & 0.08 & 10 & 2 \\
				Blue & 0.012 & 110 & 3 \\
				Red & 0.005 & 1110 & 4 \\
				Green & 0.002 & 11110 & 5 \\
				Yellow & 0.001 & 11111 & 5
			\end{tabular}
		\end{minipage}
		\caption{TODO}
		\label{fig:huffman_code}
	\end{figure*}
	
	\paragraph{c)} % What is the mean length of a code word?
	The \textit{mean length of a code word} can be computed by weighting the length of the Huffman codes obtained for each color with the respective probabilities and summing them up:
	\begin{align*}
	\text{mean} 
	&= 0.9 \cdot 1 	 &(=0.9) \\
	&+ 0.08 \cdot 2  &(=0.16) \\
	&+ 0.012 \cdot 3 &(=0.036) \\ 
	&+ 0.005 \cdot 4 &(=0.02) \\ 
	&+ 0.002 \cdot 5 &(=0.01) \\ 
	&+ 0.001 \cdot 5 &(=0.005) \\ 
	&= 1.131
	\end{align*}
	See \Cref{fig:huffman_code} for the values.
	
	\paragraph{d)} % What is the redundancy of the following 4-bit code?
	The redundancy $r$ of the given 4-bit code can be computed like so:
	\begin{align*}
	r = b - H
	\end{align*}
	Here, $b$ is the number of bits used for each pixel, which is four in this case, and $H$ is the entropy of the pixel source, which we computed above already. Thus, we obtain:
	\begin{align*}
		r = 4 - 0.570998 = 3.429002
	\end{align*}
	
	\subsection{Image Segmentation}
	
	\paragraph{a)} % When may simple threshold-based segmentation approaches be applicable for image segmentation, when not? Give at least one example for each.
	
	
	\paragraph{b)} % Does the bi-modality of the image histogram guarantee successful image segmentation if threshold-based image segmentation is applied and the threshold is selected in the minimum between the two histogram maxima?
	
	\paragraph{c)} % Explain the need of component labeling for object detection after thresholding.
	
	The need of component labeling for object detection after thresholding is due to the classification/distinguishment of pixels in each region of the image.
	
	\paragraph{d)} % What is the most typical problem in edge detection approaches? How does the Canny-algorithm try to enhance the segmentation quality?

	The most common problem in edge detection aaproaches is its sensitvity to noise, which makes edge detection inaccurate.
	
	Canny-Algorithm:
	What Canny-algorithm does is that it takes the sobel operator and use is to thin the orientation of the edge, and then use the thresholding to find dominant edges in the image.
	The optimality of canny edge detection consists of the following criterion:
	1) The detection, which indicates all the important edges should not be missed.
	2) The localization, which minimizes the distance between the actual and the located position of the egdes.
	3) The one response, which minimizes multiple reponses to a single edge. Hence, reduces noise.
	
	
	\section{Practical Problems}
	
	\subsection{(Lossy) Image Compression}
	
	\paragraph{a)} See file \texttt{task\_2\_1.py}.
	
	\vspace{12pt}
	\todo[inline]{Sibel@Nour: I changed a little bit in your implementation. You forgot to sort the eigenvectors according to their corresponding eigenvalues and take the eigenvectors for the three largest eigenvalues. Then, I renamed some of the variables with more expressive names, because I started to get confused. I hope that's ok. :)}
	
	\paragraph{b)} -
	
	\vspace{12pt}
	\todo[inline]{Sibel: I have an idea for this part, will add it soon!}
	
	\subsection{Operators for Edge Detection}
	
\end{document}
